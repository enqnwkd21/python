# =============================================================================
# ######### 파이썬분석_1025 필기
# =============================================================================
# -*- coding: utf-8 -*-
run profile1
# 인공 지능 > 머신 러닝(기계학습) > 딥러닝

# =============================================================================
# 머신 러닝(기계가 하는 학습)

# 1. 지도 학습 : 예측 할 Y가 존재
#  1) 회귀분석 : Y가 연속형 (숫자형 데이터 - 중간 값이 존재)
#  2) 분류분석 : Y가 분류형 (범주형/factor형 데이터 - 중간 값 존재 X -> 남/여, 학년)
# 2. 비지도 학습 : 예측 할 Y가 존재 X
#  1) 군집 분석
#  2) 연관 분석

# 분류 분석
# Y가 존재하는 지도 학습
# Y의 범주를 예측하는 것이 목적(예. 게임이탈여부 예측, 사망여부 예측, 사기여부 예측, 숫자/글자인식)

# 예측 분석 과정
# 1. 분석 목적 설계
# 2. 데이터 수집
# 3. 전처리(데이터 정제(결측치 처리, 이상치 처리, 잡음 제거), 변수선택, stack된 데이터일때만 분석 가능, 변수변환, 변수 스케일링, ...)
# 4. 모델링
# 5. 평가
# 6. 재설계
# 7. 모델 적용

# =============================================================================
# 데이터 셋

from sklearn.datasets import load_iris
iris = load_iris()
type(iris)         # sklearn.utils.Bunch => 사실상 딕셔너리 형태
iris.keys()        # dict_keys(['data', 'target','target_names', 'DESCR', 'feature_names']) 

iris['data']          # 설명 변수 데이터 셋(array)
iris['target']        # 종속 변수 데이터 셋(array) 모델 자체가 문자 변수의 학습을 허용 X => 종속변수가 이미 숫자로 변경 => 0,1,2로 들어옴
iris['target_names']  # 종속 변수 이름(0,1,2 : 순서대로 0 = setosa, 1 = versicolor, 2 = virginia 의미)
iris['feature_names'] # 설명 변수 이름

print(iris['DESCR'])  # print없이 출력하면 \n까지 다 출력

# =============================================================================
# knn

# Y가 범주형인 지도 학습의 형태
# 거리기반 모델 
# 결측치와 이상치에 민감
# 변수 스케일링 필요 (국어 점수에서의 1점, 수학 점수에서의 1점의 차이는 다름)
# 모델링에 사용되는 변수의 조합 중요!
# 이산형 데이터가 많을수록 불리

# 0. 모듈 호출(sklearn.neighbors은 외우기)
from sklearn.neighbors import KNeighborsClassifier as knn
import sklearn.neighbors  # dir(sklearn.neighbors) 하고 함수 가져오기

# 1. data set split (test하기 위해) (sklearn.model_selection 외우기)
# train_test_split 함수를 사용하여 train data set과 test data set으로 나눔
# 기본적으로 75 : 25 비율로 추출

from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(iris['data'],      # 원본의 설명변수
                                                    iris['target'],    # 원본의 종속변수
                                                    random_state = 0)  # random 규칙을 고정시키겠다(0이라는 숫자는 의미X, 시드값을 특정 숫자로 정하고 기준값으로 해서 랜덤하게 나름 규칙적으로 샘플, 추출되는 데이터 셋은 고정)
                                                                       


train_x, test_x, train_y, test_y = train_test_split(iris['data'],     # 원본의 설명변수
                                                    iris['target'],   # 원본의 종속변수
                                                    train_size = 0.7, # 비율 조정 가능
                                                    random_state = 0) 

test_x.shape  # 112 rows

# -- cancer 예시
cancer = pd.read_csv('cancer.csv')
cancer_x = cancer.iloc[:,2:]
cancer_y = cancer.iloc[:,1]

train_x, test_x, train_y, test_y = train_test_split(cancer_x,    # 원본의 설명변수
                                                    cancer_y,    # 원본의 종속변수
                                                    random_state = 0) 


# 2. modeling
m_knn = knn(n_neighbors = 3)   # 모델 정의 단계 : 빈 모델 형성
                               # k의 개수(n_neighbors)에 따라 예측 정확도가 달라질 수 있음
m_knn.fit(train_x, train_y)    # 모델에 데이터셋 훈련(훈련용 데이터 셋 사용) = 한 건 한 건 데이터한테 학습 시킴(train data set으로만 해야 함, 덮어쓰기 X)
m_knn.score(test_x, test_y)    # 모델에 대한 평가(평가용 데이터 셋을 사용) 

# [ 참고 - score 메서드 내부 연산 ]
(m_knn.predict(test_x) == test_y).sum() / test_y.shape[0] * 100
(m_knn.predict(test_x) == test_y).sum() / len(test_x) * 100


# 3. predict (전달되는 데이터는 2차원 형태!)
new_data = np.array([3.6, 5.9, 1.3, 0.7])
m_knn.predict(new_data)                # Error : predict에서 요구하는 data set은 2차원인데 new_data는 1차원
m_knn.predict(new_data.reshape(1,-1))  # 1,4로 맞춰준 것 =>  array([0]) : 0번째 Y값!, 
                                       # but 만약에 new_data = np.array([[3.6, 5.9, 1.3, 0.7]]) 2차원으로 만들어줬으면    
                                       # reshape할 필요 X
iris['target_names'][0]
# -- vpre가 더 있어보이니깐~
vpre = m_knn.predict(new_data.reshape(1,-1)) 
iris['target_names'][vpre][0]          # 실제 이름 확인 => [0]를 전달하는 이유는  array(['setosa'], dtype='<U10') -> 'setosa'



# 4. 튜닝
# 최적의 k의 수 확인 !! 
# (정수 포지션일때만 확인하면 됨, train이 더 높고, train과 test gap이 별로 없는게 best)

# -- cancer 예시
cancer = pd.read_csv('cancer.csv')
cancer_x = cancer.iloc[:,2:]
cancer_y = cancer.iloc[:,1]

train_x, test_x, train_y, test_y = train_test_split(cancer_x,    # 원본의 설명변수
                                                    cancer_y,    # 원본의 종속변수
                                                    random_state = 0) 
vscore_tr = [];vscore_te = []
for i in range(1, 11) :
    m_knn = knn(n_neighbors = i)  
    m_knn.fit(train_x, train_y)   
    vscore_tr.append(m_knn.score(train_x, train_y)) 
    vscore_te.append(m_knn.score(test_x, test_y)) 
    
plt.plot(range(1, 11), vscore_tr, label = 'train_score', c = 'r')
plt.plot(range(1, 11), vscore_te, label = 'test_score', c = 'b')
plt.legend()

# [ 연습 문제 ]
# 변수를 선택하고 변수 스케일링을 한 후 knn적용 전/후 비교


# =============================================================================
#  Decision Tree

# Y가 범주형인 지도 학습
# 트리기반 모델
# 이상치, 결측치에 덜 민감
# 변수 스케일링 불필요
# 모델 내부에서 변수 중요도 측정 => 변수 선택 기준
# 이산형(숫자지만 factor을 가지고 있음), 연속형 데이터에 대해서도 잘 동작

# 0. 모듈 호출 (import sklearn.tree모듈 외우기)
import sklearn.tree
dir(sklearn.tree)
from sklearn.tree import DecisionTreeClassifier as dt_c

# 1. 데이터 분리
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(iris['data'],  iris['target'],  random_state = 0)  

# 2. 모델링 (불순도를 가장 낮게 만드는 것을 발견)
m_dt = dt_c()
m_dt.fit(train_x, train_y)
m_dt.score(test_x, test_y)   # 97.37

# -- 변수 산점도
plt.scatter(iris['data'][:,2], iris['data'][:,3], c =iris['target'] )
plt.xlabel(iris['feature_names'][2])
plt.ylabel(iris['feature_names'][3])


# 3. 튜닝
# 1) max_depth
# 2) min_samples_split : 최소 가지치기 기준(split할 수 있는 최소 기준)
#    min_samples_split 값보다 노드의 오분류 데이터의 수가 많으면 추가 가지치기 진행 !
#    min_samples_split 값이 낮을수록 추가 가지치기를 진행할 가능성이 높아짐에 따라 
#    보다 복잡한 모델이 생성 (예측력 강화, 과대적합 발생 가능성 높아짐)  

# 4. 변수 중요도 확인
m_dt.feature_importances_

# 5. 시각화
# 1) graphviz 설치(window)
# down 주소 => https://graphviz.gitlab.io/_pages/Download/Download_windows.html
# down 후 압축해제(C:/Program Files (x86))

# 2) graphviz 설치(python)
# pip install graphviz(cmd에서 수행)

# 3) python path 설정
import os
os.environ['PATH'] += os.pathsep + 'C:/Program Files/Graphviz/bin'

import graphviz

from sklearn.tree import export_graphviz
export_graphviz(m_dt,                           # 모델명 
                out_file="tree.dot", 
                class_names=load_iris().target_names,
                feature_names=load_iris().feature_names, 
                impurity=False, 
                filled=True)

with open("tree.dot", encoding='UTF8') as f:
    dot_graph = f.read()

g1 = graphviz.Source(dot_graph)
g1.render('dt_1', cleanup=True)



# [ 연습 문제 - DT의 max_depth의 튜닝 결과를 시각화]
train_x, test_x, train_y, test_y = train_test_split(iris['data'],  iris['target'],  random_state = 99)  
m_dt = dt_c()
m_dt.fit(train_x, train_y)
m_dt.score(train_x, train_y)  # 100
m_dt.score(test_x, test_y)    # 97.36


score_tr = [];score_te = []
for i in range(2, 11) :
    m_dt = dt_c(max_depth = i)
    m_dt.fit(train_x, train_y) 
    score_tr.append(m_dt.score(train_x, train_y)) 
    score_te.append(m_dt.score(test_x, test_y)) 
    
plt.plot(range(2, 11), score_tr, label = 'train_score', c = 'r')
plt.plot(range(2, 11), score_te, label = 'test_score', c = 'b')
plt.legend()

m_dt = dt_c(max_depth = 3)
m_dt.fit(train_x, train_y)
m_dt.score(train_x, train_y)  # 98.21
m_dt.score(test_x, test_y)


# =============================================================================
# ######### 파이썬분석_1026 필기
# =============================================================================

# 교차 검증 (cross validation) 

# 평가점수가 계속 달라지기 때문에(원본 데이터로부터 train/test 분리하는 과정에서 랜덤하게 선택되므로)
# 평가점수를 일반화 할 필요가 있다! => 여러번 수행 후 평균으로 최종 점수 확인
# -- 목적 : 모델을 평가하려고 할때 원본 데이터를 train/test용으로 나누게 됩니다. 그 과정에서
# 랜덤하게 데이터가 선택이 되면서 매 평가할 때 마다 계속 다른 데이터 셋이 적용이 되고, 따라서
# 매 평가마다 다른 평가 점수가 나오기 때문에 이렇게 달라지는 평가 점수를 여러번 평가를 수행해서
# 그 점수의 평균으로 일반화 시키기 위한 목적입니다.

from sklearn.model_selection import cross_val_score as cv
cv(estimator,   # 적용하려는 모델
   X,           # 설명 변수 데이터 셋
   y,           # 종속 변수 데이터 셋
   cv)          # k값(교차검증 횟수, test data set의 비율 : 1/cv)

# [ 예제 - cancer data를 사용한 decision tree 적용(with CV)]
run profile1
# 1. data loading
cancer = pd.read_csv('cancer.csv')
cancer_x = cancer.iloc[:,2:]
cancer_y = cancer.iloc[:,1]

# 2. data split
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(cancer_x, cancer_y, random_state=0)

# 3. modeling
from sklearn.tree import DecisionTreeClassifier as dt_c
m_dt = dt_c()
m_dt.fit(train_x, train_y)
m_dt.score(test_x, test_y) # 88.11 점 정말 믿을 수 있겠어? => cross validation 실행

# 4. cross validation
cv_score = cv(m_dt, cancer_x, cancer_y, cv = 5) 

cv_score = cv(m_dt,        # 적용하려는 모델
              cancer_x,    # 설명 변수 데이터 셋(train_x는 내가 나눈 것)
              cancer_y,    # 종속 변수 데이터 셋
              cv = 5) 

cv_score.mean()            # 91.56 이 5번 실행 후의 평균 => 88.11 보다 신뢰가는 점수

# 5. max_depth 튜닝(with CV)
# step1) train/test split
train_x, test_x, train_y, test_y = train_test_split(cancer_x, cancer_y, random_state=0)

# step2) max_depth 튜닝 과정에서 평가 진행 시 train data set 으로 교차 검증 수행
vscore = []
for i in range(2, 11) :
    m_dt = dt_c(max_depth = i)
    cv_score = cv(m_dt, train_x, train_y, cv = 3)   # validation score = train data set을 또 3분할(3번 반복) 
                                                    # => val 점수만 알려줌 train 점수 모름 -> 과대적합 못 봄
                                                    # validation data set은 max_depth가 적정한지를 보기 위한 data set
    vscore.append(cv_score.mean())    # max_depth 변화에 따른 최종 점수
    
    
plt.plot(range(2, 11), vscore)

# step3) 일반화 된 튜닝과정에서의 평가점수를 토대로 best parameter 결정
m_dt = dt_c(max_depth = 6)

# step4) 최종 평가 : 원래의 train data set(best parameter 반영, validation data set이 이미 포함되어있음) 으로
#        훈련 후 test data set 으로 최종 평가 수행
m_dt.fit(train_x, train_y)
m_dt.score(test_x, test_y)   


# =============================================================================
# Random Forest
# 의사결정나무가 여러개인 분류 모델
# 앙상블 알고리즘(여러 모델이 혼합되어 하나의 예측 결과를 가져다 주는 방식, 예.knn + pca)
# 복원 추출을 허용하여 원본 데이터셋에서 랜덤하게 선택된 데이터를 각 트리에 학습 = (부트스트랩)
# 각 트리의 결과를 결합하여(다수결 혹은 평균) 최종 결론을 내는 방식 = (배깅)
# 각 노드를 결정하는 설명 변수의 후보의 개수 - 임의노드최적화(in R : mtry, in python : max_features)
# 설명변수의 후보의 개수가 클수록 서로 비슷한 트리가 구성될 확률이 높다
# 설명변수의 후보의 개수가 클수록 트리 상관성이 높아진다(임의성 정도가 클수록 트리상관성이 높아짐)

# 0. module loading
from sklearn.ensemble import RandomForestClassifier as rf_c

import sklearn.ensemble
dir(sklearn.ensemble)

# 1. data loading, split
cancer = pd.read_csv('cancer.csv')
cancer_x = cancer.iloc[:,2:]
cancer_y = cancer.iloc[:,1]
train_x, test_x, train_y, test_y = train_test_split(cancer_x, cancer_y, random_state=0)

# 2. modeling
m_rf = rf_c()
m_rf.fit(train_x, train_y)
m_rf.score(train_x, train_y) # 100
m_rf.score(test_x, test_y)   # 97.90

# with CV
cv_score = cv(m_rf, cancer_x, cancer_y)
cv_score.mean()              # 96.31

# 3. tunning
# 1) max_depth : 트리의 크기, 클수록 복잡한 모델 생성(예측력 강화, but 과대적합 문제 발생 가능성 높아짐)
# 2) min_samples_split : 최소 가지치기 기준(작을수록 더 복잡한 모델 생성)
# 3) max_features : 임의성 정도(각 노드 결정시 설명변수의 후보 수, 클수록 서로 비슷한 트리 구성)
    
rf_c(n_estimators,      # 트리수(default : 100)
     criterion,         # 불순도(default : gini)
     max_depth,
     min_samples_split, # 가지치기를 할 최소 기준의 수
     max_features =,    # np.sqrt(n_features) 자동으로 설명 변수의 개수 기반으로 고정(1~설명변수개수)
     n_jobs = )         # 병렬처리를 위한 옵션  

vscore_tr =[];vscore_te =[]
for i in range(1,31) :
    m_rf = rf_c(max_features=i)
    m_rf.fit(train_x, train_y)
    vscore_tr.append(m_rf.score(train_x, train_y))
    vscore_te.append(m_rf.score(test_x, test_y))

import matplotlib.pyplot as plt
plt.plot(range(1,31), vscore_tr, label = 'train_score', c = 'r')
plt.plot(range(1,31), vscore_te, label = 'test_score', c = 'b')
plt.legend


# 4. 변수 중요도
cancer_x.columns
s1 = Series(m_rf.feature_importances_, index = cancer_x.columns)
s1.sort_values(ascending = False)

s1.plot(kind = 'barh')  # 'barh' 수평 bar plot


# =============================================================================
# ######### 파이썬분석_1027 필기
# =============================================================================
# DT < RF < GBM < XGB
run profile1
# =============================================================================
# 부스팅
# 이전 모델의 오차를 보완하는 학습 형태
# 약한 모델로 부터 강한 학습기를 만드는 과정
# 에이다부스트(AdaBoost), 그래디언트부스트(GradientBoosting)
# 에이다부스트 : 오분류 데이터에 가중치를 수정하여 이전 트리의 오차를 보완하는 헝태
# 그래디언트부스트 : 사전 가지치기를 통해 이전 트리의 오차를 보완하는 형태 (오차를 최소화하는 가지치기를 반복)

from sklearn.ensemble import GradientBoostingClassifier as gbm

# 1. data loading, split
cancer = pd.read_csv('cancer.csv')
cancer_x = cancer.iloc[:,2:]
cancer_y = cancer.iloc[:,1]
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(cancer_x, cancer_y, random_state=0)

# 2. modeling
m_gbm = gbm()
m_gbm.fit(train_x, train_y)
m_gbm.score(test_x, test_y) # 97.20

# 3. tunning
# 1) n_estimators : 트리 수
vscore_tr =[];vscore_te =[]
for i in range(1,101) :
    m_gbm = gbm(n_estimators=i)
    m_gbm.fit(train_x, train_y)
    vscore_tr.append(m_gbm.score(train_x, train_y))
    vscore_te.append(m_gbm.score(test_x, test_y))
    
import matplotlib.pyplot as plt
plt.plot(range(1,101), vscore_tr, label = 'train_score', c = 'r')
plt.plot(range(1,101), vscore_te, label = 'test_score', c = 'b')
plt.legend()


from sklearn.model_selection import cross_val_score as cv
vscore = []
for i in range(1,101) :
    m_gbm = gbm(n_estimators=i)
    m_gbm.fit(train_x, train_y)
    cv_result = cv(m_gbm, cancer_x, cancer_y, cv = 5)
    vscore.append(cv_result.mean())
    
plt.plot(range(1,101), vscore, c = 'b')
plt.legend()   


m_gbm = gbm(n_estimators=20)
m_gbm.fit(train_x, train_y)
m_gbm.score(test_x, test_y)  # 96.50


# 2) learning_rate : 학습률 (0 ~ 1 사이값)
# 오차 보완 정도
# 클수록 오차 보완 정도가 강화

vscore_tr =[];vscore_te =[]
for i in [0.001, 0.01, 0.1, 1] :
    m_gbm = gbm(learning_rate=i)
    m_gbm.fit(train_x, train_y)
    vscore_tr.append(m_gbm.score(train_x, train_y))
    vscore_te.append(m_gbm.score(test_x, test_y))
    
import matplotlib.pyplot as plt
plt.plot([0.001, 0.01, 0.1, 1], vscore_tr, label = 'train_score', c = 'r')
plt.plot([0.001, 0.01, 0.1, 1], vscore_te, label = 'test_score', c = 'b')
plt.legend()

# -- best : 0.1

# =============================================================================
# XGB
# pip install xgboost
from xgboost.sklearn import XGBClassifier as xgb


# 변수 스케일링
# 설명변수(x만 스케일링)의 서로 다른 범위(분산)을 동일한 범주내에서 비교하기 위한 목적
# 거리기반 모델, 계수 크기 비교, 신경망, PCA, SVM 등의 모델 필요
# standard scaling : (x - x.mean()) / x.std()
# minmax scaling : (x - x.min()) / (x.max() - x.min())

from sklearn.preprocessing import StandardScaler as standard
from sklearn.preprocessing import MinMaxScaler as minmax

# cancer data set scaling
m_sc = standard()
m_sc.fit(cancer_x)        # 스케일링에 필요한 정보 연산(평균, 표준편차), 실제 x 데이터는 아직 변환 안됨
m_sc.transform(cancer_x)  # 위 데이터를 사용하여 실제 변환 수행
cancer_x_sc = m_sc.transform(cancer_x)

# scaling시 주의 사항
# 이미 train / test data set이 분리된 상황

# case1) 서로 같은 기준으로 train / test 적용 or 아예 애초에 원본을 먼저 스케일링을 하고 나누기
m_sc = minmax()
m_sc.fit(train_x)
train_x_sc = m_sc.transform(train_x)
test_x_sc = m_sc.transform(test_x)

train_x_sc.min()  # 0
train_x_sc.max()  # 1

test_x_sc.min()   # -0.00079 : 0으로 규격화 X
test_x_sc.max()   # 1.36 : 1으로 규격화 X

# case2) 서로 같은 기준으로 train / test 각각 적용 _ 잘못된 방법
m_sc1 = minmax()
m_sc2 = minmax()

m_sc1.fit(train_x)
m_sc2.fit(test_x)

train_x_sc2 = m_sc1.transform(train_x)
test_x_sc2 = m_sc2.transform(test_x)

train_x_sc2.min()  # 0으로 규격화
test_x_sc2.max()   # 1으로 규격화

test_x_sc2.min()   # 0으로 규격화 
train_x_sc2.max()  # 1으로 규격화

# 올바른 스케일링 기법 비교를 위한 시각화
# 1) fig, subplot 생성
fig, ax = plt.subplots(1,3)

# 2) 원본 데이터 산점도
# 스케일링 작업은 원본 데이터가 갖는 본질적 특성(데이터간의 거리)은 유치한 채
# 데이터의 범위만 수정하는 작업이므로 
# 올바른 스케일링 후 데이터는 원본의 분포(산점도)와 동일해야 함  

import mglearn     # pip install mglearn으로 설치 필요
plt.rc('font', family = 'Malgun Gothic')

fig, ax = plt.subplots(1,3)
ax[0].scatter(train_x.iloc[:,0], train_x.iloc[:,1], c=mglearn.cm2(0), label = 'train')
ax[0].scatter(test_x.iloc[:,0], test_x.iloc[:,1], c=mglearn.cm2(1), label = 'test')
ax[0].legend()
ax[0].set_title('원본 산점도')
ax[0].set_xlabel(train_x.columns[0])
ax[0].set_ylabel(train_x.columns[1])

# 3) 올바른 스케일링 데이터 산점도
ax[1].scatter(train_x_sc[:,0], train_x_sc[:,1], c=mglearn.cm2(0), label = 'train')
ax[1].scatter(test_x_sc[:,0], test_x_sc[:,1], c=mglearn.cm2(1), label = 'test')
ax[1].legend()
ax[1].set_title('올바른 스케일링 산점도', fontsize = 10)
ax[1].set_xlabel(train_x.columns[0])
ax[1].set_ylabel(train_x.columns[1])

# 4) 잘못된 스케일링 데이터 산점도
# 원본의 데이터가 가지고 있는 데이터간의 특징(속성, 거리)을 그대로 유지하지 못한 체 스케일링이 진행 됨
ax[2].scatter(train_x_sc2[:,0], train_x_sc2[:,1], c=mglearn.cm2(0), label = 'train')
ax[2].scatter(test_x_sc2[:,0], test_x_sc2[:,1], c=mglearn.cm2(1), label = 'test')
ax[2].legend()
ax[2].set_title('잘못된 스케일링 산점도', fontsize = 10)
ax[2].set_xlabel(train_x.columns[0])
ax[2].set_ylabel(train_x.columns[1])


# =============================================================================
# ######### 파이썬분석_1028 필기
# =============================================================================
run profile1
# 변수 변환
# 원본 데이터로부터 보다 의미있는 변수를 가공(파생변수)
# 로그변환, 지수변환, 거듭제곱변환, 역수변환, 교차곱
# 예) 게임 이탈 예측 : kill/death, kill/hour, kill/games


# interaction(교호작용, 상호작용) 효과 검증
# 교차곱에 대해 변수변환 후 의미있는 신호가 있는지 확인

# 0. 모듈 호출
from sklearn.preprocessing import PolynomialFeatures as poly

# 1. data loading
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# 2. data split
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(iris_x, iris_y, random_state=0)

# 3. data scaling
from sklearn.preprocessing import StandardScaler as standard
from sklearn.preprocessing import MinMaxScaler as minmax

m_sc = standard()
m_sc.fit(train_x)       
train_x_sc = m_sc.transform(train_x) 
test_x_sc = m_sc.transform(test_x)

# 4. interaction
m_poly = poly()       # degree = 2 (이차항만 고려)
m_poly.fit(train_x)   # 발생가능한 설명변수 조합을 그려넣는 행위 확인 중 
                      # => 설명변수의 개수는 train/test같기 때문에 test_x써도 결과는 같음
                      # => fitting할때는 train만 하고 transform은 train/test둘다 사용 가능
                             
train_x_sc_poly = m_poly.transform(train_x_sc) # 위에서 확인한 변수 조합대로 실제 변수 변환 수행
test_x_sc_poly = m_poly.transform(test_x_sc)

m_poly.get_feature_names()   # 변수 조합 출력
poly_cols = m_poly.get_feature_names(iris['feature_names'])  # 변수 조합 출력 with 설명 변수 이름까지

# -- 보기 좋게 dataframe으로 만듦
df_train_poly = DataFrame(train_x_sc_poly, columns = poly_cols)  # 첫번째 컬럼의 상수항(1)은 의미 X
df_test_poly = DataFrame(test_x_sc_poly, columns = poly_cols)    

# 5. 변수 중요도 확인
from sklearn.ensemble import RandomForestClassifier as rf_c

m_rf = rf_c()
m_rf.fit(train_x_sc_poly, train_y)
m_rf.score(test_x_sc_poly, test_y)  # 97.36

s1 = Series(m_rf.feature_importances_, index = poly_cols)
s1.sort_values(ascending = False)
# -- x1 X x2 가 개별의 x1, x2보다 중요도가 높을 때 연구 필요
# -- x1, x1 X x2, .... , x2일때도 연구 필요

# [ 연습 문제 - cancer data에서의 의미있는 교차곱 형태 유무 확인 ]
from sklearn.datasets import load_breast_cancer

# 1. data loading
cancer = load_breast_cancer()
cancer_x = cancer['data']
cancer_y = cancer['target']

# 2. data split
train_x, test_x, train_y, test_y = train_test_split(cancer_x, cancer_y, random_state=0)

# 3. data scaling
from sklearn.preprocessing import StandardScaler as standard
from sklearn.preprocessing import MinMaxScaler as minmax

m_sc = standard()
m_sc.fit(train_x)
train_x_sc = m_sc.transform(train_x)
test_x_sc = m_sc.transform(test_x)

# 4. interaction
m_poly = poly()                                   # degree=2(이차항만 고려)
m_poly.fit(train_x_sc)                            # 발생가능한 변수 조합 확인
train_x_sc_poly = m_poly.transform(train_x_sc)    # 위에서 확인한 변수 조합대로 실제 변수 변환 수행
test_x_sc_poly = m_poly.transform(test_x_sc) 

m_poly.get_feature_names()
vcols = Series(cancer['feature_names']).str.replace(' ','_')  # 변수와 곱 쉽게 식별하기 위해
poly_cols = m_poly.get_feature_names(vcols)


# 5. 변수 중요도 확인
from sklearn.ensemble import RandomForestClassifier as rf_c

m_rf = rf_c()
m_rf.fit(train_x_sc_poly, train_y)
m_rf.score(test_x_sc_poly, test_y)    # 96.50

s1 = Series(m_rf.feature_importances_, index=poly_cols)
s1.sort_values(ascending=False)


# 변수 선택(feature selection)
# 1. 필터 기법 : 모델에 의존하지 않은 상태로 변수 자체의 평가(변수 자체가 갖는 통계량 기반)로 변수를 선택/제거 방식
# - 상관계수, 공분산, IG, gini index, entropy index, ...

# 2. 래퍼 기법 : 변수의 일부를 모데링 => 결과를 토대로 변수 선택의 여부 결정
# - 일변량 통계(단변량 선택 - univariate selection) : 변수 하나씩 종속변수와의 상관관계를 측정하여 변수를 선택하는 방식
# - RFE : 전체 변수를 삽입하여 모델링 한 후 모델링 결과를 가지고 중요도가 낮은 변수를 제거하는 방식

# 3. 임베디드 기법 : 모엘의 정확도에 기여하는 변수를 학습하여 점차 적은수의 설명변수를 갖도록 제어(규제)
#                 - 지도학습 모델을 통해서 변수를 선택
# - SelectFromModel : 트리기반, 회귀기반 모델을 통해 모델 자체가 갖는 변수 중요도를 파악하여 변수를 선택하는 방식
# - 릿지, 라쏘

# 예제 - iris data set을 사용한 변수 선택 과정

# step1) data loading
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# step2) 잡음 데이터 추가(10개 추가)
vrandom = np.random.RandomState(0)
vdata = vrandom.normal(size=(150,10))
pd.concat([DataFrame(iris_x), DataFrame(vdata)], axis = 1)   # pandas 결합 함수
iris_total = np.hstack([iris_x, vdata])                      # numpy 결합 함수(hstack : 수평, vstack : 수직)

# step3) 변수 선택(SelectFromModel)
from sklearn.feature_selection import SelectFromModel
m_rf = rf_c()
m_fs = SelectFromModel(m_rf, threshold = 'mean')  # 각 변수들의 중요도를 확인하고 변수 중요도의 평균보다도 높은 애들만 고르겠다는 문법
                                                  # threshold = '' 문법은 무조건 ~보다 높은 걸 선택->중요한 애들을 선택하는거니깐, 상위
m_fs.fit(iris_total, iris_y)

m_fs.get_support()               # 선택된 변수 확인
iris_total[:,m_fs.get_support()] # 선택된 변수만 추출
m_fs.transform(iris_total)       # 첫번째, 세번째, 네번째 설명 변수 추출

# step4) 선택된 변수 시각화
import matplotlib.pyplot as plt
plt.matshow(m_fs.get_support().reshape(1,-1), cmap ='gray_r') # -- 색으로 칠해진 부분이 선택된 부분(주요한 부분)

# step5) 변수 중요도 확인
m_fs.estimator_.feature_importances_
m_fs.estimator_.feature_importances_ > m_fs.estimator_.feature_importances_.mean() # 변수 중요도의 평균보다도 높은 애들만 고르겠다


# =============================================================================
# ######### 파이썬분석_1029 필기
# =============================================================================

# 예제 - iris data set을 사용한 변수 선택 과정
run profile1
# =============================================================================
# 1. 일변량 선택
# 변수 하나와 종속변수와의 관계를 파악하여 변수를 선택하는 방식
# 다른 변수와 함께 학습되는 경우를 고려하지 못하여 전체 학습 결과와 다른 결과가 나올 수 있음
# 가장 간단한 방식, 연산 속도가 가장 빠름
# 상관계수, 불순도 척도(gini index, entropy index, IG, 카이제곱통계량)

# step1) data loading
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# step2) 잡음 데이터 추가(10개 추가)
vrandom = np.random.RandomState(0)
vdata = vrandom.normal(size=(150,10))
pd.concat([DataFrame(iris_x), DataFrame(vdata)], axis = 1)   # pandas 결합 함수
iris_total = np.hstack([iris_x, vdata])                      # numpy 결합 함수(hstack : 수평, vstack : 수직)

# step3) 변수 선택(SelectKBest, SelectPercentile)
from sklearn.feature_selection import SelectKBest, SelectPercentile

m_fs = SelectKBest(k=4)          # 가장 베스트 조합의 4개 변수 선택, default가 10개 선택(k=10)
m_fs.fit(iris_total, iris_y)

m_fs.get_support()               # 선택된 변수 확인 
iris_total[:,m_fs.get_support()] # 선택된 변수만 추출
m_fs.transform(iris_total)       # 첫번째, 세번째, 네번째 설명 변수 추출

# step4) 선택된 변수 시각화
import matplotlib.pyplot as plt
plt.matshow(m_fs.get_support().reshape(1,-1), cmap ='gray_r') # -- 색으로 칠해진 부분이 선택된 부분(주요한 부분)
plt.rc('axes', unicode_minus=False)                           # -- 마이너스 기호 깨짐 현상 없애기

# step5) 변수 중요도 확인
m_fs.scores_

# =============================================================================
# 2. 모델 기반 선택(임베디드 기법)
# 지도학습 머신러닝 모델을 사용하여 변수 선택
# 일변량 분석과는 다르게 모든 특성(변수)을 고려하므로 변수들간의 상호 관계 반영
# 일변량 선택 기법보다는 내부 연산 과정 복잡, 더 많은 시간 소요
# 트리기반 모델 선택 시 변수 중요도를 측정, 회귀기반 모델 선택 시 회귀 계수를 측정 

# step1) data loading
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# step2) 잡음 데이터 추가(10개 추가)
vrandom = np.random.RandomState(0)
vdata = vrandom.normal(size=(150,10))
pd.concat([DataFrame(iris_x), DataFrame(vdata)], axis = 1)   # pandas 결합 함수
iris_total = np.hstack([iris_x, vdata])                      # numpy 결합 함수(hstack : 수평, vstack : 수직)

# step3) 변수 선택(SelectFromModel)
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as rf_c
m_rf = rf_c()
m_fs = SelectFromModel(m_rf, threshold = 'mean')  # 각 변수들의 중요도를 확인하고 변수 중요도의 평균보다도 높은 애들만 고르겠다는 문법
                                                  # threshold = '' 문법은 무조건 ~보다 높은 걸 선택->중요한 애들을 선택하는거니깐, 상위
m_fs.fit(iris_total, iris_y)

m_fs.get_support()               # 선택된 변수 확인
iris_total[:,m_fs.get_support()] # 선택된 변수만 추출
m_fs.transform(iris_total)       # 첫번째, 세번째, 네번째 설명 변수 추출

# step4) 선택된 변수 시각화
import matplotlib.pyplot as plt
plt.matshow(m_fs.get_support().reshape(1,-1), cmap ='gray_r') # -- 색으로 칠해진 부분이 선택된 부분(주요한 부분)


# step5) 변수 중요도 확인
m_fs.estimator_.feature_importances_
m_fs.estimator_.feature_importances_ > m_fs.estimator_.feature_importances_.mean() # 변수 중요도의 평균보다도 높은 애들만 고르겠다


# =============================================================================
# 3. RFE(Recursive Feature Elimination):래퍼 기법(모델기반)

# step1) data loading
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# step2) 잡음 데이터 추가(10개 추가)
vrandom = np.random.RandomState(0)
vdata = vrandom.normal(size=(150,10))
pd.concat([DataFrame(iris_x), DataFrame(vdata)], axis = 1)   # pandas 결합 함수
iris_total = np.hstack([iris_x, vdata])                      # numpy 결합 함수(hstack : 수평, vstack : 수직)

# step3) 변수 선택(RFE)
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier as rf_c
m_rf = rf_c()
m_fs = RFE(m_rf, n_features_to_select=4)    # 4개 변수 선택
m_fs.fit(iris_total, iris_y)

m_fs.get_support()               # 선택된 변수 확인
iris_total[:,m_fs.get_support()] # 선택된 변수만 추출
m_fs.transform(iris_total)      

# step4) 선택된 변수 시각화
import matplotlib.pyplot as plt
plt.matshow(m_fs.get_support().reshape(1,-1), cmap ='gray_r') # -- 색으로 칠해진 부분이 선택된 부분(주요한 부분)


# step5) 변수 중요도 확인
m_fs.estimator_.feature_importances_
m_fs.ranking_



# [ 연습 문제 - cancer data에서의 중요 변수 확인(순위) ]
# 1. data loading
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
cancer_x = cancer['data']
cancer_y = cancer['target']
                    
# 2. feature selection
from sklearn.feature_selection import SelectFromModel, SelectKBest, SelectPercentile, RFE
from sklearn.ensemble import RandomForestClassifier as rf_c

# -- 모델 생성
m_fs1 = SelectKBest(k=15)  
m_fs2 = SelectFromModel(rf_c(), threshold = 'mean') 
m_fs3 = RFE(rf_c(), n_features_to_select=30)   # 전체 순위를 보고싶기 때문에 전체(30) 

# -- 데이터 훈련
m_fs1.fit(cancer_x, cancer_y) 
m_fs2.fit(cancer_x, cancer_y)
m_fs3.fit(cancer_x, cancer_y)

# -- 변수 중요도(score)확인
vscore1 = Series(m_fs1.scores_, index = cancer['feature_names'])
vscore2 = Series(m_fs2.estimator_.feature_importances_, index = cancer['feature_names'])
vscore3 = Series(m_fs3.estimator_.feature_importances_, index = cancer['feature_names'])

# -- 순위 매기기
vrank1 = vscore1.rank(ascending = False).astype('int').reset_index()
vrank2 = vscore2.rank(ascending = False).astype('int').reset_index()
vrank3 = vscore3.rank(ascending = False).astype('int').reset_index()

# -- 컬럼 이름 수정
vrank1.columns =['feature_name', 'rank']
vrank2.columns =['feature_name', 'rank']
vrank3.columns =['feature_name', 'rank']

# -- 순위 기반 조인
imsi = pd.merge(vrank1, vrank2, on = 'rank')
total = pd.merge(imsi, vrank3, on = 'rank')

# -- 데이터프레임 한번에 보기 
rank_total = total.sort_values(by = 'rank').iloc[:,[0,2,3]]
rank_total.columns = ['일변량','모델기반','RFE']

# -- 인덱스 : rownumber -> 순위(1~30)으로 수정
rank_total.index = np.arange(1,31)


# =============================================================================
# 차원 축소
# 단순히 변수를 제거하는 것이 아니라(제거된 변수의 정보를 사용하지 않겠다는 의미)
# 기존 변수를 모두 활용하여(기존 변수의 정보를 모두 사용)
# 새로운 의미있는 인공변수를 유도(유도되는 인공변수의 수가 원래의 차원의 수보다 줄어드는 상황)
# 2차원, 3차원 공간안에 전체 데이터의 분포를 시각화 목적
# 차원축소 전에는 반드시 스케일링이 필요

# 1. 주성분분석(PCA) - 의미 있는 변수들의 가공
# - 기존 데이터들이 갖는 분산에 집중하여 분산을 유지하는 방식의 변수 결합
# - 첫번째 유도되는 인공변수(주성분)가 기존 데이터의 분산을 가장 많이 설명
# - 두번째 유도되는 인공변수는 첫번째 주성분이 설명하지 못한 분산에 대해 설명
# - 회귀분석의 다중공선성의 문제를 해결하기 위한 용도로도 자주 등장
# - 주성분 수 결정 : 분산설명력이 80% 이상이 되는 주성분의 수 확인
# - 단점 : 유도된 인공변수의 해석이 어려움

# c1 : 68%
# c2 : 6%
# c3 : 4%
# c4 : 1%
# ==> 누적 분산 설명력이 c3까지 오면 80% 이상이 되기 때문에 유도된 인공변수 3개 선택

# [ 예제 - iris data 차원 축소 (PCA)]
# 1. data loading 
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# 2. data scaling
from sklearn.preprocessing import StandardScaler as standard
m_sc = standard()     
iris_x_sc = m_sc.fit_transform(iris_x) 

# 3. PCA(비지도학습)
from sklearn.decomposition import PCA
m_pca = PCA(n_components = 2)                # 2개 인공변수 유도 (빈 모델)
m_pca.fit(iris_x_sc)                         # 인공변수 유도 식 계산 (비지도 학습(Y값 입력X), 분산 설명력)  
iris_x_sc_pca = m_pca.transform(iris_x_sc)   # 계산된 수식으로 실제 변수 변환(4개를 2개 인공변수로 바꾼 결과)

# 유도된 인공변수 확인
# 1) 각 계수 확인
m_pca.components_
# array([[ 0.52106591, -0.26934744,  0.5804131 ,  0.56485654],
#        [ 0.37741762,  0.92329566,  0.02449161,  0.06694199]])

# 2) 위 계수에 의해 추정된 인공변수 유도식 PCA(n_components = 2)라고 지정했으니 2행, IRIS설명변수가 4개니, 2X4
# C1 = 0.52106591*X1 + -0.26934744*X2 + 0.5804131*X3  + 0.56485654*X4
# C2 = 0.37741762*X1 +    92329566*X2 + 0.02449161*X3 + 0.06694199*X4

# 3) 위 식에 의해 변환된 데이터 셋
iris_x_sc[0,:] => iris_x_sc_pca[0,:]

0.52106591*iris_x_sc[0,0] +  -0.26934744*iris_x_sc[0,1] + 0.5804131*iris_x_sc[0,2] +  0.56485654*iris_x_sc[0,3]
= -2.264702813233667 = iris_x_sc_pca[0,0]

0.37741762*iris_x_sc[0,0] +  0.92329566*iris_x_sc[0,1] + 0.02449161*iris_x_sc[0,2] +  0.06694199*iris_x_sc[0,3]
= 0.48002658778021806 = iris_x_sc_pca[0,1]

# 4. 시각화
import mglearn
mglearn.discrete_scatter(iris_x_sc_pca[:,0], iris_x_sc_pca[:,1], y = iris_y)


# =============================================================================
# ######### 파이썬분석_1101 필기
# =============================================================================
run profile1

# [ 예제 - iris data 차원 축소(PCA) ]
# 1. data loading
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# 2. data scaling
from sklearn.preprocessing import StandardScaler as standard
m_sc = standard()
iris_x_sc = m_sc.fit_transform(iris_x)

# 3. PCA
from sklearn.decomposition import PCA
m_pca = PCA(n_components=3)                        # 2개 인공변수 유도
m_pca.fit(iris_x_sc)                               # 인공변수 유도 식 계산
iris_x_sc_pca = m_pca.transform(iris_x_sc)         # 계산된 수식으로 실제 변수 변환

# 유도된 인공변수 확인
# 1) 각 계수 확인
m_pca.components_


# 유도된 인공변수의 분산 설명력
m_pca.explained_variance_
m_pca.explained_variance_ratio_      # 유도된 각 인공변수의 설명력(%)
                                     # 첫번째와 두번째 인공변수로 이미 80%이상의 분산설명력을 보이므로
                                     # iris data를 PCA에 의해 의미 있는 인공변수를 유도할 경우 2개로 충분

# 4. 시각화
from mpl_toolkits.mplot3d import Axes3D, axes3d
pig1 = plt.figure()
ax = Axes3D(pig1)

ax.scatter(iris_x_sc_pca[:,0],     # x축 좌표
           iris_x_sc_pca[:,1],     # y축 좌표
           iris_x_sc_pca[:,2],     # z축 좌표
           c = iris_y,             # 색 표현(Y마다 다른 색)
           cmap = plt.spring(),    # 색 색깔(어떤색이냐, 팔레트)
           s = 60,                 # 점 크기
           edgecolors = 'k'        # 점 테두리 색
           )

# [ 연습 문제 - cancer data set을 사용하여 PCA로 유도된 인공변수로 2,3차원 시각화 ]
# 1. data loading
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
cancer_x = cancer['data']
cancer_y = cancer['target']

# 2. data scaling
from sklearn.preprocessing import StandardScaler as standard
m_sc = standard()
cancer_x_sc = m_sc.fit_transform(cancer_x)

# 3. PCA
from sklearn.decomposition import PCA
m_pca1 = PCA(n_components=2)                        
m_pca2 = PCA(n_components=3)  

m_pca1.fit(cancer_x_sc)  
m_pca2.fit(cancer_x_sc) 
                            
cancer_x_sc_pca1 = m_pca1.transform(cancer_x_sc)     
cancer_x_sc_pca2 = m_pca2.transform(cancer_x_sc)     

# 유도된 인공변수 확인
# 1) 각 계수 확인
m_pca1.components_
m_pca2.components_


# 유도된 인공변수의 분산 설명력
m_pca1.explained_variance_ratio_     
m_pca2.explained_variance_ratio_                                   


# 4. 시각화
# 1) 2차원 시각화
import mglearn
mglearn.discrete_scatter(cancer_x_sc_pca1[:,0], cancer_x_sc_pca1[:,1], y = cancer_y)


# 2) 3차원 시각화
from mpl_toolkits.mplot3d import Axes3D, axes3d
pig1 = plt.figure()
ax = Axes3D(pig1)

ax.scatter(cancer_x_sc_pca2[:,0],     # x축 좌표
           cancer_x_sc_pca2[:,1],     # y축 좌표
           cancer_x_sc_pca2[:,2],     # z축 좌표
           c = cancer_y,             # 색 표현(Y마다 다른 색)
           cmap = plt.spring(),      # 색 색깔(어떤색이냐, 팔레트)
           s = 60,                   # 점 크기
           edgecolors = 'k'          # 점 테두리 색
           )


# [ 연습 문제 - PCA + knn ]
from sklearn.neighbors import KNeighborsClassifier as knn

# 1. data loading
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
cancer_x = cancer['data']
cancer_y = cancer['target']

# 2. data scaling
from sklearn.preprocessing import StandardScaler as standard
m_sc = standard()
cancer_x_sc = m_sc.fit_transform(cancer_x)

# 3. PCA
from sklearn.decomposition import PCA
m_pca = PCA(n_components=30)                        
m_pca.fit(cancer_x_sc)                            
cancer_x_sc_pca = m_pca.transform(cancer_x_sc)  

m_pca.explained_variance_ratio_.cumsum()       # 각 인공변수 설명력의 누적합
   
# 4. knn 모델링
from sklearn.neighbors import KNeighborsClassifier as knn
# 5개의 인공변수 유도
m_pca = PCA(n_components=5)
m_pca.fit(cancer_x_sc)                            
cancer_x_sc = m_pca.transform(cancer_x_sc)  

# 데이터분리(5개의 인공변수를 선택한 후, 데이터 분리)
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(cancer_x_sc, cancer_y, random_state = 0)

# 모델 훈련 및 평가
m_knn = knn()
m_knn.fit(train_x, train_y)
m_knn.score(test_x, test_y)   # 93.00

# --
# 분산은 제곱이 된 상태이기 때문에 개별 숫자의 평균으로부터의 편차가 클수록 더 커지는 효과를 주게된다. 
# 즉, 평균에서 거리가 더 먼 숫자일 수록 편차에 더 가중치를 주어
# 계산된 상태가 되는 이는 데이터분석에서 데이터의 분포나 패턴을 볼 때 더 뚜렷하게 볼 수 있게 해주기도 한다.

# 다중공선성의 문제
# 원인 : 설명변수끼리의 상관성
# 문제 : 회귀분석의 결과 신뢰도 떨어짐(유도된 계수 크기 부정확, 부호가 반대)
# 해결 : 변수제거, 변수결합, PCA, 릿지, 라쏘, ...


# =============================================================================
# ######### 파이썬분석_1102 필기
# =============================================================================
run profile1

# 차원축소
# PCA

# MDS(다차원척도법)
# - 비지도학습
# - 기존 데이터들이 갖는 분포적 특성(데이터간 거리)을 최대한 유지하는 인공변수 유도
# - 다차원 척도법에 의해 유도된 인공변수 평가 : 기존의 데이터간 거리와 유도된 변수로 계산된 거리의차이(STRESS)
# - stress가 클수록 좋지 않은 모델

# [ 예제 - iris data set을 사용하여 차원축소(MDS) ]
# 1. data loading
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# 2. data scaling
from sklearn.preprocessing import StandardScaler as standard
m_sc = standard()
iris_x_sc = m_sc.fit_transform(iris_x)

# 3. MDS
from sklearn.manifold import MDS

# 1) 2개 인공변수 유도
m_mds1 = MDS(n_components=2, random_state=0)        
iris_x_sc_mds1 = m_mds1.fit_transform(iris_x_sc)                    
   
# 2) 3개 인공변수 유도
m_mds2 = MDS(n_components=3, random_state=0)        
iris_x_sc_mds2 = m_mds2.fit_transform(iris_x_sc) 

# 4. 시각화
# 1) 2개 인공변수 시각화
import mglearn
mglearn.discrete_scatter(iris_x_sc_mds1[:,1], iris_x_sc_mds1[:,0], y = iris_y)


# 2) 3개 인공변수 시각화
from mpl_toolkits.mplot3d import Axes3D, axes3d
pig1 = plt.figure()
ax = Axes3D(pig1)

ax.scatter(iris_x_sc_mds2[:,0],    
           iris_x_sc_mds2[:,1],     
           iris_x_sc_mds2[:,2],     
           c = iris_y,             
           cmap = plt.spring(),     
           s = 60,                  
           edgecolors = 'k')

# 5. 평가(stress) - 절대적 평가 기준이 없음 (데이터가 많을수록 커짐) => elbow point 확인 용도
#                 - 몇개의 인공변수를 유도할지 보기 위해 스트레스를 계산 
m_mds1.stress_    # 235.945
m_mds2.stress_    # 10.509

# sklearn stress
import numpy as np
from sklearn.metrics import euclidean_distances  # 유클리디안 거리

DA = euclidean_distances(iris_x)             # 원본데이터의 거리
DE1= euclidean_distances(iris_x_sc_mds1)     # 변환된 데이터 셋끼리의 거리
DE2= euclidean_distances(iris_x_sc_mds2)

stress1 = np.sum((DE1 - DA)**2)*0.5         # 3521.2042
stress2 = np.sum((DE2 - DA)**2)*0.5         # 3378.1498

# kruskal stress(절대적 평가 기준 마련)
stress1/(0.5 * np.sum(DA**2))        # 2개로 유도된 인공변수의 stress 지수 => 0.034
stress2/(0.5 * np.sum(DA**2))        # 3개로 유도된 인공변수의 stress 지수 => 0.033

# 0 < 0.025 : 완벽
#   < 0.05  : 매우 좋음
#   < 0.1   : 좋음
#   < 0.2   : 보통
#   > 0.2   : 나쁨
 

# [ 연습 문제 = cancer data set을 사용하여 차원축소(MDS)]
# 1. data loading
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
cancer_x = cancer['data']
cancer_y = cancer['target']

# 2. data scaling
from sklearn.preprocessing import StandardScaler as standard
m_sc = standard()
cancer_x_sc = m_sc.fit_transform(cancer_x)

# 3. MDS
from sklearn.manifold import MDS


m_mds1 = MDS(n_components=2, random_state=0)    
m_mds2 = MDS(n_components=3, random_state=0) 
    
cancer_x_sc_mds1 = m_mds1.fit_transform(cancer_x_sc)                    
cancer_x_sc_mds2 = m_mds2.fit_transform(cancer_x_sc) 

# 4. 시각화
import mglearn
mglearn.discrete_scatter(cancer_x_sc_mds1[:,0], cancer_x_sc_mds1[:,1], y = cancer_y)


# 2) 3개 인공변수 시각화
from mpl_toolkits.mplot3d import Axes3D, axes3d
pig1 = plt.figure()
ax = Axes3D(pig1)

ax.scatter(cancer_x_sc_mds2[:,0],    
           cancer_x_sc_mds2[:,1],     
           cancer_x_sc_mds2[:,2],     
           c = cancer_y,             
           cmap = plt.spring(),     
           s = 60,                  
           edgecolors = 'k')

# 5. 적절한 인공변수 수 확인(stress계산)
# 1) model stress
vstress_mo = [];vstress_kr = []
for i in range(1,31):
    m_mds = MDS(n_components=i, random_state=0)                      
    cancer_x_sc_mds = m_mds.fit_transform(cancer_x_sc) 
    
    # model stress 확인
    vstress_mo.append(m_mds.stress_)
 
    # kruskal stress 확인
    DA = euclidean_distances(cancer_x)             
    DE = euclidean_distances(cancer_x_sc_mds)     
    
    stress = np.sum((DE - DA)**2)*0.5  
    vstress_kr.append(stress / (np.sum(DA**2)*0.5))   # 거리의 표준화 => 절대적 기준치 출력 가능

# elbow point 확인
plt.plot(range(1,31), vstress_mo)  # 5시점 이후부터는 급격한 변화는 확인 X => 5~7개수의 인공변수 유도 적절!!

# kruskal_stress 확인
df_stress = DataFrame({'n_components': np.arange(1,31),'kruskal_stress':vstress_kr})

plt.plot(range(1,31), vstress_kr)

# =============================================================================
########## 파이썬분석_1103 필기
# =============================================================================
run profile1
# SVM 
# - 원래는 분류 목적으로 수행하는 예측 모델링 기법이나 회귀를 수행해주기도 함
# - 분류 기준(분류선, 분류평면)을 찾는 모델, 분류선으로 부터 거리(마진)이 커야 분류가 good
# - 분류선과 분류평면을 만들기 위해 오차를 최소화하는 각 기울기와 절편 구함(회귀와 유사)
# - 예측력이 강하고 과적합이 되기 쉽지 않은 모델
# - 계산 수식이 너무 복잡, 해석 불가, 학습 비용이 큰 모델
# - 학습되는 변수의 형태에 큰 영향받음(사전 변수 선택 필수)
# - 변수 스케일링 필요
# - 이상치에 민감


# 용어 정리
# 1. support vector(지지 벡터) : 분류기준을 지지하는 데이터포인트
# 2. 초평면 : 다차원일때(3차원 이상) 분류 기준이 되는 평면(판)
# 3. 결정정계 : 두 클래스를 분류하는 경계
# 4. 마진 : 초평면으로부터의 서포트 벡터까지의 거리
# 5. 슬랙 변수 : 완벽한 분류가 불가능할 때 선형적으로 분류하기 위해허용되는 오차를 위한 변수

# 예제 - SVM 초평면 유도 과정
# 1. data loading
from sklearn.datasets import make_blobs
X, y = make_blobs(centers=4, random_state=8)

X.shape     # 100 X 2 (2개의 설명변수)
y           # 0,1,2,3의 값을 가짐

y = y % 2   # 이진 분류 모델을 만들기 위한 작업(2로 나눈 나머지) 
            # => 이진 분류기가 가장 효과적이라는 것이지 다항분류여도 충분히 분류 가능


# 2. 데이터 분포 확인
import mglearn
mglearn.discrete_scatter(X[:,0], X[:,1], y)   # x축, y축에 둘 값, color = y(0,1)

# 3. 선형 분류기를 통해 분류 시도
from sklearn.svm import LinearSVC
m_scv = LinearSVC()
m_scv.fit(X,y)           # warning : n_features(설명변수) < n_samples(샘플)

mglearn.plots.plot_2d_separator(m_scv, X)

# 4. 3차원 데이터로 변경 후 시각화
X[:,1] ** 2   # 두번째 컬럼의 제곱항을 추가하기 위해 가공 
              # X[:,0], X[:,1] 변수를 활용하여 하나의 변수로 추가하기 
              # => 3차원으로 만들어야하니 설명변수 3개( X[:,0], X[:,1],X[:,1]**2) 필요
 
np.hstack([X, X[:,1]**2)       # 에러, 2차원 데이터와 1차원 데이터의 결합 불가
np.hstack([X, X[:,1:2]**2])    # 차원축소 방지를 위한 슬라이스 색인으로 처리
np.hstack([X, X[:,1].reshape(-1,1)**2]) 
pd.concat([DataFrame(X), DataFrame(X[:,1]**2)], axis = 1)

X_new = np.hstack([X, X[:,1:2]**2])

# 시각화
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D, axes3d

fig = plt.figure()
ax = Axes3D(fig)

# -- y가 0인 경우
ax.scatter(X_new[y==0,0], X_new[y==0,1], X_new[y==0,2],
           c = 'b', cmap = mglearn.cm2, s = 60, edgecolors='k')    # 2차원의 파란색 애들

# -- y가 1인 경우
ax.scatter(X_new[y==1,0], X_new[y==1,1], X_new[y==1,2],
           c = 'r', cmap = mglearn.cm2, s = 60, edgecolors='k',    # 2차원의 주황 애들
           marker = '^')


# 5. SVM 적용
m_scv2 = LinearSVC()
l_svm = m_scv2.fit(X_new, y)

coef = l_svm.coef_.ravel()      # SVM 모델에 의해 추정된 계수 확인(ravel:1차원으로 표현)
intercept = l_svm.intercept_    # SVM 모델에 의해 추정된 절편 확인

# 6. 초평면 시각화
np.linspace(-3,3,100)          # -3부터 3까지 100개를 균등분할
xx = np.linspace(X_new[:,0].min()-2, X_new[:,0].max()+2, 50)       
yy = np.linspace(X_new[:,1].min()-2, X_new[:,1].max()+2, 50)       

XX, YY = np.meshgrid(xx, yy)   # 2차원 공간안의 좌표값으로 설정

coef[0] * XX + coef[1]*YY + intercept
ZZ = (coef[0] * XX + coef[1]*YY + intercept) / -coef[2]  # 규격화, 높낮이 설정
     
# 시각화
fig = plt.figure()
ax = Axes3D(fig)

ax.plot_surface(XX, YY, ZZ, alpha = 0.3)         # 평면 시각화(판 삽입)

# -- y가 0인 경우
ax.scatter(X_new[y==0,0], X_new[y==0,1], X_new[y==0,2],
           c = 'b', cmap = mglearn.cm2, s = 60, edgecolors='k')    # 2차원의 파란색 애들

# -- y가 1인 경우
ax.scatter(X_new[y==1,0], X_new[y==1,1], X_new[y==1,2],
           c = 'r', cmap = mglearn.cm2, s = 60, edgecolors='k',    # 2차원의 주황 애들
           marker = '^')

# [ 번외 - 아래 질문들에 대한 확인 ]
# 두 개의 설명변수가 갖는 발생가능한 이차항의 변수를 모두 고려시 가장 중요한 변수가 무엇인지 확인!

# 학생1 : 선생님 3차원 데이터로 변경할때 X[:,1] ** 2 쓰는 건 사용자 마음인거죠? X[:,0] x X[:,1]로 변수 추가 해도 되는건가요?
# interaction(교호작용, 상호작용) 효과 검증
# 교차곱에 대해 변수변환 후 의미있는 신호가 있는지 확인

m_poly = poly()       
X_poly = m_poly.fit_transform(X)    
                             
m_poly.get_feature_names()   # ['1', 'x0', 'x1', 'x0^2', 'x0 x1', 'x1^2']

from sklearn.ensemble import RandomForestClassifier as rf_c

m_rf = rf_c()
m_rf.fit(X_poly, y)
m_rf.score(X_poly, y) 

m_rf.feature_importances_
# array([0., 0.04620513, 0.12264404, 0.06241693, 0.33392748, 0.43480642])





# =============================================================================
########## 파이썬분석_1104 필기
# =============================================================================
# SVM 매개변수
# - C : 데이터포인트에 대한 중요도(가중치) 제어, 클수록 높은 가중치를 부여 => 복잡한 경계 생성
# - gamma : 결정경계를 만드는데 영향을 주는 데이터의 범위
#           값이 커질수록 데이터의 범위를 줄임(결정경계 인근 데이터만 고려) => 복잡한 경계 생성
            
# 예제 - iris data 
# 1. data loading
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

# 2. modeling
# 1) data split
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(iris_x, iris_y, random_state=0)

# 2) training
from sklearn.svm import LinearSVC
from sklearn.svm import SVC

m_lsvc = LinearSVC()
m_svc = SVC()

m_lsvc.fit(train_x, train_y)
m_svc.fit(train_x, train_y)

m_lsvc.score(test_x, test_y)     # 92.1
m_svc.score(test_x, test_y)      # 97.3

m_svc.predict_proba(test_x)      # 에러발생, probability=False일때 출력 불가 => 각 클래스별 확률 출력

m_svc = SVC(probability=True)
m_svc.fit(train_x, train_y)
m_svc.predict_proba(test_x)

# 3) tuning
# 3-1) C만 튜닝
vscore_tr = [] ; vscore_te = []
for i in [0.001, 0.01, 0.1, 1, 10, 100] :
    m_svc = SVC(C=i)
    m_svc.fit(train_x, train_y)
    vscore_tr.append(m_svc.score(train_x, train_y))
    vscore_te.append(m_svc.score(test_x, test_y))

import matplotlib.pyplot as plt
plt.plot(vscore_tr, label='train', c='r')
plt.plot(vscore_te, label='test', c='b')
plt.legend()
plt.xticks(np.arange(0,6), [0.001, 0.01, 0.1, 1, 10, 100])

# 3-2) C, gamma 동시 튜닝
outlist = []
for i in [0.001, 0.01, 0.1, 1, 10, 100, 1000] :
    inlist=[]
    for j in [0.001, 0.01, 0.1, 1, 10, 100] :
        m_svc = SVC(C=i, gamma=j)
        m_svc.fit(train_x, train_y)
        inlist.append(m_svc.score(test_x, test_y))
    outlist.append(inlist)
    
vscore = np.array(outlist)        
df1 = DataFrame(vscore, index=[0.001, 0.01, 0.1, 1, 10, 100], columns=[0.001, 0.01, 0.1, 1, 10, 100])
df1.index.name = 'C'
df1.columns.name = 'gamma'


# 시각화       
import mglearn 
from mglearn.tools import heatmap
heatmap(values,          # 2차원 숫자 데이터
        xlabel,          # x축 이름
        ylabel,          # y축 이름
        xticklabels,     # x축 눈금 이름
        yticklabels,     # y축 눈금 이름
        cmap)            # color map

vtick = [0.001, 0.01, 0.1, 1, 10, 100]
vheatmap = heatmap(vscore, 'gamma', 'C', vtick, vtick, cmap='viridis')
plt.colorbar(vheatmap)

# 기타 히트맵
# 1) matplotlib
plt.rc('font', family='Malgun Gothic')
plt.pcolor(vscore)

def f_annot(data) :
    nrow = data.shape[0]
    ncol = data.shape[1]
    for i in range(0,nrow) :
        for j in range(0,ncol) :
            plt.text(j + 0.5, i + 0.5 , '%.2f' % vscore[i,j], ha = 'center', va='center')

f_annot(vscore)

plt.xticks(np.arange(0,6), vtick)
plt.yticks(np.arange(0,6), vtick)
plt.xlabel('gamma')
plt.ylabel('C')
plt.title('매개변수 변화에 따른 예측률')
plt.colorbar()         
        
# 2) seaborn
import seaborn as sns
sns.heatmap(vscore, annot=True, cmap='viridis')
plt.xticks(np.arange(0,6), vtick)
plt.yticks(np.arange(0,6), vtick)
plt.xlabel('gamma')
plt.ylabel('C')
plt.title('매개변수 변화에 따른 예측률')


# [ 연습 문제 - cancer data 를 사용하여 SVC 모델 적용 ]



# =============================================================================
########## 파이썬분석_1108 필기
# =============================================================================
# 회귀 분석
# 지도 학습(Y가 연속형)
# 인과 관계 분석
# 통계적 가정(정규성, 등분산성, 선형성, 독립성, ...)을 필요로 하는 모델 => 어려움
# 통계적 가정을 만족시키지 못하는 경우 회귀분석 결과 신뢰도 떨어짐
# 통계적 모델이므로 모델에 대한 통계적 평가 메트릭 존재(R^2, 회귀계수나 모형의 유의성 검정)

# 회귀분석 가정
# 1. 선형성 : 각 설명변수와 종속변수와의 관계
# 2. 등분산성 : 잔차(오차)의 분산이 일정 => 잔차산점도로 확인 가능!
# 3. 정규성 : 잔차(오차)항이 정규분포를 따름 ~ N(0, sigma^2) => qqplot
# 4. 독립성 : 잔차(오차)항이 서로 독립적이다(자기 상관이 없음) => 잔차산점도 확인
 

# 모형의 검정(F검정)
# H0 : 모형이 유의하지 않다
# H1 : 모형이 유의하다

# 회귀 계수의 검정(T검정)
# H0 : b1 = 0
# H1 : b1 != 0

# 모형의 설명력
# F검정으로 인해 모형이 유의하고 T검정 결과에서 모든 회귀 계수가 유의하더라도
# 모형 자체가 갖는 설명력이 낮은 경우 모형 재검토 필요!
# R^2, 수정된 R^2로 평가(주로 85% 이상)


# [ 예제 - 보스턴 주택 가격 셋을 사용한 회귀 분석 수행 ]
# 1. data loading
from sklearn.datasets import load_boston
boston = load_boston()
boston_x = boston['data']
boston_y = boston['target']

print(boston['DESCR'])

# 2. data split
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(boston_x, boston_y, random_state=0)

# 3. modeling
# 1) sklearn 모듈
from sklearn.linear_model import LinearRegression
m_lm = LinearRegression()
m_lm.fit(train_x, train_y)

m_lm.score(train_x, train_y)    # R^2 값 리턴
m_lm.score(test_x, test_y)      # R^2 값 리턴

# 2) statsmodels 모듈
from statsmodels.formula.api import ols
df1 = DataFrame(boston_x, columns = boston['feature_names'])
df2 = DataFrame(boston_y, columns = ['PRICE'])

df_boston = pd.concat([df1,df2], axis=1)

vfm = 'PRICE ~ ' + '+'.join(boston.feature_names)
m_lm2 = ols(vfm, data = df_boston).fit()

m_lm2.resid                           # 잔차
print(m_lm2.summary())                # 회귀분석표
# => INDUS, AGE 컬럼 두 변수가 유의하지 않게 나옴!!(제거 고려)


# 3) random forest regressor
from sklearn.ensemble import RandomForestRegressor as rf_r

m_rfr = rf_r()
m_rfr.fit(train_x, train_y)

m_rfr.score(train_x, train_y)         # 98.37
m_rfr.score(test_x, test_y)           # 76.31

# SVR
from sklearn.preprocessing import StandardScaler as stand
m_sc = stand()
train_x_sc = m_sc.fit_transform(train_x)
test_x_sc = m_sc.transform(test_x)

from sklearn.svm import SVR
m_svr = SVR()
m_svr.fit(train_x_sc, train_y)
m_svr.score(train_x_sc, train_y)   
m_svr.score(test_x_sc, test_y)   

# tuning
vscore_tr = [] ; vscore_te = []
for i in [0.001, 0.01, 0.1, 1, 10, 100, 1000] :
    m_svr = SVR(C=i)
    m_svr.fit(train_x_sc, train_y)
    vscore_tr.append(m_svr.score(train_x_sc, train_y))
    vscore_te.append(m_svr.score(test_x_sc, test_y))

import matplotlib.pyplot as plt
plt.plot(vscore_tr, c='red', label='train')
plt.plot(vscore_te, c='blue', label='test')
plt.xticks(np.arange(0,7), [0.001, 0.01, 0.1, 1, 10, 100, 1000])
plt.legend()

# C = 100
m_svr = SVR(C=100)
m_svr.fit(train_x_sc, train_y)
m_svr.score(train_x_sc, train_y)    # 97.83   
m_svr.score(test_x_sc, test_y)      # 78.55

# 변수 제거
boston_x = df1.drop(['INDUS', 'AGE'], axis=1).values
train_x, test_x, train_y, test_y = train_test_split(boston_x, boston_y, random_state=0)

m_sc = stand()
train_x_sc = m_sc.fit_transform(train_x)
test_x_sc = m_sc.transform(test_x)

m_svr = SVR(C=100)
m_svr.fit(train_x_sc, train_y)
m_svr.score(train_x_sc, train_y)    # 97.83   
m_svr.score(test_x_sc, test_y)      # 78.55


# =============================================================================
########## 파이썬분석_1109 필기
# =============================================================================
# 회귀 분석 진단
# 1. data laoding
from sklearn.datasets import load_boston
boston = load_boston()
boston.keys()
boston_x = boston['data']
boston_y = boston['target']

boston_corr = DataFrame(boston_x, columns = boston['feature_names']).corr()

# 히트맵 시각화
import seaborn as sb
sb.heatmap(boston_corr, annot=True, cmap='Reds')

# 2. 다중공선성 확인
# VIF = 1 / (1-R^2)
# 10 이상이면 다중공선성 확인!
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
variance_inflation_factor(exog,          # 원본 데이터
                          exog_idx       # VIF 측정하고자 하는 설명변수 위치)

vvif = [ vif(boston_x, i) for i in range(boston_x.shape[1]) ]

Series(vvif, index=boston['feature_names'])
    
# PCA + Regressor
from sklearn.preprocessing import StandardScaler as standard
m_sc1 = standard()
boston_x_sc = m_sc1.fit_transform(boston_x)

# PCA 변환
from sklearn.decomposition import PCA

# 주성분 수 정하기
pca_ratio = []
for i in range(1,13) :
    m_pca = PCA(i)
    boston_x_sc_pca = m_pca.fit_transform(boston_x_sc)
    pca_ratio.append(m_pca.explained_variance_ratio_.sum())

# 주성분 유도
m_pca = PCA(6)
boston_x_sc_pca = m_pca.fit_transform(boston_x_sc)
df_pca = DataFrame(boston_x_sc_pca, columns = [ 'X' + i for i in np.arange(0,6).astype('str')])
df_pca_total = pd.concat([df_pca, df2], axis=1)

# 인공변수끼리의 상관관계 확인
DataFrame(df_pca).corr()
sb.heatmap(DataFrame(df_pca).corr(), annot=True, cmap='Reds')

# 인공변수로 회귀분석 모델링
vfm = 'PRICE ~ ' + '+'.join(df_pca.columns)
m_lm2 = ols(vfm, data = df_pca_total).fit()

m_lm2.resid     # 잔차
print(m_lm2.summary())

# VIF
[ variance_inflation_factor(df_pca.values, i) for i in range(df_pca.shape[1])]

# SVR
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(df_pca, df2)

from sklearn.svm import SVR
m_svr = SVR(C=100)
m_svr.fit(train_x, train_y)
m_svr.score(train_x, train_y)   
m_svr.score(test_x, test_y)   

# 회귀분석의 가정 확인(검정)
# 1) 등분산성 : 잔차가 0을 기준으로 패턴없이 널리 흩어져있는 정도 확인
m_lm2.fittedvalues                       # 예측값, 적합값
m_lm2.resid                              # 일반 잔차
m_lm2.outlier_test()['student_resid']    # 스튜던트잔차(표준화된 잔차)

import matplotlib.pyplot as plt
plt.scatter(m_lm2.fittedvalues, m_lm2.resid)
plt.scatter(m_lm2.fittedvalues, m_lm2.outlier_test()['student_resid'])

# 참고 - 이상치 검정(H0 : 각 관측치가 이상치가 아니다)
df_outlier = m_lm2.outlier_test()
df_outlier.loc[df_outlier['bonf(p)'] < 0.05, :]

# bonf(p) = Bonferroni p-value : 한번에 여러 가설검정 수행 시 p-value를 각 하나의 가설에 맞게
#                                재계산 한 값

                               
# 2) 정규성 : 잔차가 정규분포를 따르는지 여부 확인 => 히스토그램, 커널밀도함수, qqplot, 샤피로검정
# - 히스토그램
v1 = np.random.normal(0, 1, 150)
plt.hist(m_lm2.resid)             # 잔차 히스토그램
plt.hist(v1)                      # 잔차 히스토그램

# - 히스토그램 + 커널밀도함수
import seaborn 
seaborn.distplot(m_lm2.resid)
seaborn.distplot(v1)

# qqplot
from scipy import stats
stats.probplot(m_lm2.resid, plot=plt)
stats.probplot(v1, plot=plt)

# shapiro test
# H0 : 모집단이 정규분포를 따른다
stats.shapiro(m_lm2.resid)
stats.shapiro(v1)

# 3) 선형성 : 독립변수와 종속변수간의 일정 패턴이 존재함을 검정 => 산점도
df1 = DataFrame(boston_x, columns = boston.feature_names)
df2 = DataFrame(boston_y, columns = ['PRICE'])
df_boston = pd.concat([df1,df2], axis=1)

pd.plotting.scatter_matrix(df_boston, hist_kwds = {'bins' : 20})


# 4) 잔차 자기상관성 => 잔차산점도, Durbin watson test
print(m_lm2.summary())         # Durbin-Watson : 0.967

from statsmodels.stats.stattools import durbin_watson
durbin_watson(m_lm2.resid)     # 0.96   
                               # 0 ~ 4의 값을 갖고, 0과 4에 가까울수록 자기상관이 강하다
                               #                   2에 가까울수록 자기상관이 없다
                               # => 정확한 진단은 durbin watson table 확인(n, p에 따라 달라짐)
                               # n = 506(훈련 데이터 수), p = 13(설명변수의 개수)
                               
# 0        dL        dU      2       4-dU         4-dL         4
#  자기상관    미결정     자기상관없음       미결정       자기상관


# =============================================================================
########## 파이썬분석_1110 필기
# =============================================================================

# 기타 회귀 모델(릿지, 라쏘, 엘라스틱넷)
# 회귀 모델에서 다중공선성의 문제를 해결하기 위한 용도로 주로 사용
# 가중치 규제(L1, L2 규제)를 통해 회귀 모델을 제어하는 형식
# 가중치 규제 강도(alpha)의 조절을 통해 모델의 복잡도 제어
# alpha가 클수록 가중치가 작아짐

from sklearn.datasets import load_boston
boston = load_boston()
boston_x = boston['data']
boston_y = boston['target']

from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(boston_x, boston_y, random_state=0)

# 1. 릿지(Ridge)
# L2 규제를 사용한 가중치 제어
# 각 가중치의 제곱의 합을 최소화 하는 방식

from sklearn.linear_model import Ridge
m_rg = Ridge()                  # alpha = 1.0
m_rg.fit(train_x, train_y)
m_rg.score(test_x, test_y)      # 62.66

m_rg2 = Ridge(alpha = 0.001)
m_rg3 = Ridge(alpha = 100)

m_rg2.fit(train_x, train_y)
m_rg3.fit(train_x, train_y)

m_rg2.score(test_x, test_y)   # alpha = 0.001(기존1보다 작은값 => 규제완화 => 가중치 크기 커짐 => 모델 복잡도 증가)
m_rg3.score(test_x, test_y)   # alpha = 100(기존값보다 큰값 => 규제강화 => 계수 크기 작아짐 => 모델 복잡도 감소)

# 계수 확인
m_rg.coef_         # 설명변수에 대한 회귀 계수
m_rg3.coef_         # 설명변수에 대한 회귀 계수

m_rg.intercept_    # 절편

import matplotlib.pyplot as plt
plt.plot(m_rg.coef_, '^', label='ridge alpha 1')
plt.plot(m_rg2.coef_, 'v', label='ridge alpha 0.001')
plt.plot(m_rg3.coef_, '*', label='ridge alpha 100')
plt.legend()

# alpha가 클수록 규제 강도가 강화 =>
# 설명력이 떨어지는 변수에 대한 가중치(계수)를 0에 가깝게 수정
# 더 단순한 모델 생성

# alpha에 대한 강도 확인!!
vscore_tr = [] ; vscore_te = []

for i in [0.001, 0.01, 0.1, 1, 10, 100, 1000] :
    m_rg = Ridge(alpha=i)
    m_rg.fit(train_x, train_y)
    vscore_tr.append(m_rg.score(train_x, train_y))
    vscore_te.append(m_rg.score(test_x, test_y))
    
import matplotlib.pyplot as plt    
plt.plot(vscore_tr, label='train', c='red')
plt.plot(vscore_te, label='test', c='blue')
plt.legend()
plt.xticks(np.arange(0,7), [0.001, 0.01, 0.1, 1, 10, 100, 1000])

# 2. 라쏘(Lasso)
# L1 규제를 사용한 가중치 제어
# 가중치의 절댓값의 합을 최소화 방식 규제
from sklearn.linear_model import Lasso

m_ls = Lasso()                 # alpha 1
m_ls2 = Lasso(alpha=10)                
m_ls3 = Lasso(alpha=100)               

m_ls.fit(train_x, train_y)
m_ls2.fit(train_x, train_y)
m_ls3.fit(train_x, train_y)

m_ls.score(test_x, test_y)
m_ls2.score(test_x, test_y)
m_ls3.score(test_x, test_y)

(m_ls.coef_ == 0).sum()        # 추정된 회귀 계수가 0인 변수가 2개(2개 변수의 탈락 발생)
(m_ls2.coef_ == 0).sum()       # 추정된 회귀 계수가 0인 변수가 2개(2개 변수의 탈락 발생)
(m_ls3.coef_ == 0).sum()       # 추정된 회귀 계수가 0인 변수가 2개(2개 변수의 탈락 발생)


# 3. 엘라스틱넷
# L1 + L2 규제 혼합 방식
# L1, L2 규제의 강도 조절 가능

from sklearn.linear_model import ElasticNet
m_el = ElasticNet(l1_ratio=0.5)
m_el.fit(train_x, train_y)
m_el.score(test_x, test_y)

# [ 연습문제 - 자동차 연비 예측 ]
# auto-mpg.csv 파일을 읽고 연비(mpg)를 예측하는 회귀 모델 생

# 데이터 로딩
df1 = pd.read_csv('auto-mpg.csv')
df1.dtypes
df1 = df1.drop('car name', axis=1)
df1.columns = df1.columns.str.replace(' ','_')    # 컬럼이름에 공백을 포함하는 경우 '_'로 변경

# 결측치 처리
df1['horsepower'] = df1['horsepower'].replace('?', NA)
df1['horsepower'] = df1['horsepower'].astype('float')

# NA 확인
df1.isna().sum()      # 각 컬럼별 NA의 개수
df1 = df1.dropna()    # 결측치를 포함하는 행 데이터 제거

# 데이터 탐색
pd.plotting.scatter_matrix(df1, hist_kwds = {'bins' : 20})

# data split
from sklearn.model_selection import train_test_split
df_x = df1.drop('mpg', axis=1)
df_y = df1['mpg']

train_x, test_x, train_y, test_y = train_test_split(df_x, df_y, random_state=0)

# ols fitting
from statsmodels.formula.api import ols
fml = 'mpg ~' + '+'.join(df_x.columns)
m_lm = ols(fml, data=df1).fit()
print(m_lm.summary())

# VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
vvif = [ vif(df_x.values, i) for i in range(df_x.shape[1]) ]
Series(vvif, index=df_x.columns)

# PCA
from sklearn.preprocessing import StandardScaler as standard
m_sc1 = standard()
df_x_sc = m_sc1.fit_transform(df_x)

from sklearn.decomposition import PCA
m_pca = PCA(7)
df_x_sc_pca = m_pca.fit_transform(df_x_sc)

df_x_sc_pca = DataFrame(df_x_sc_pca, columns=['C1','C2','C3','C4','C5','C6','C7'])
df_new = pd.merge(df_x_sc_pca, df_y, right_index=True, left_index=True)

fml = 'mpg ~' + '+'.join(['C1','C2','C3','C4','C5','C6','C7'])
m_lm = ols(fml, data=df_new).fit()
print(m_lm.summary())

# lasso
m_ls = Lasso()
m_ls.fit(train_x, train_y)

m_ls.score(train_x, train_y)
m_ls.score(test_x, test_y)

# alpha에 대한 강도 확인!!
vscore_tr = [] ; vscore_te = []

for i in [0.001, 0.01, 0.1, 1, 10, 100, 1000] :
    m_rg = Lasso(alpha=i)
    m_rg.fit(train_x, train_y)
    vscore_tr.append(m_rg.score(train_x, train_y))
    vscore_te.append(m_rg.score(test_x, test_y))
    
import matplotlib.pyplot as plt    
plt.plot(vscore_tr, label='train', c='red')
plt.plot(vscore_te, label='test', c='blue')
plt.legend()
plt.xticks(np.arange(0,7), [0.001, 0.01, 0.1, 1, 10, 100, 1000])

# re-modeling
m_ls = Lasso(alpha=0.01)
m_ls.fit(train_x, train_y)
m_ls.score(train_x, train_y)
m_ls.score(test_x, test_y)

m_ls.coef_


# 로지스틱 회귀
# 회귀식 추정을 통해 Y 분류 예측
# 로그 오즈(실패 대비 성공 확률)을 설명변수의 선형 합으로 추정
# 위 수식을 성공확률(P(Y=1))에 대해 정리하면 시그모이드함수(이진 클래스 분류곡선)식이 나옴


# [ 예제 - iris data를 이진 분류기로 변환한 후 단 하나의 설명변수로 품종 예측 수행 ]
from sklearn.datasets import load_iris
iris = load_iris()
iris_x = iris['data']
iris_y = iris['target']

iris_x = iris_x[iris_y != 2]        # 이진 분류기로 만들기 위한 Y가 2인 데이터 제거
iris_y = iris_y[iris_y != 2]        # 이진 분류기로 만들기 위한 Y가 2인 데이터 제거

# 1. modeling
from sklearn.linear_model import LogisticRegression
m_lr = LogisticRegression()
m_lr.fit(iris_x[:,0:1], iris_y)          # log(odds) = bo + b1X1

# 회귀 식 확인
m_lr.intercept_      # -17.23182528
m_lr.coef_           # 3.1732613 

Y=0(Setosa), Y=1(versicolor)
# X(Sepal Length)가 5.1일때 Y=1(Versicolor)일 확률은?

P(Y=1) = 1 / (1 + np.exp(-(-17.23182528 + 3.1732613*5.1)))
       = 0.2595723126255942
       
P(Y=0) = 1 - 0.2595723126255942
       = 0.7404276873744058

# => 따라서 Sepal Length가 5.1일때 꽃의 품종은 Setosa로 예상!

# 모델의 예측 결과 확인
m_lr.predict([[5.1]])          # Setosa로 예측 됨
m_lr.predict_proba([[5.1]])    # Y가 각각 0과 1의 확률 출력



# =============================================================================
# ######### 파이썬분석_1111 필기
# =============================================================================

# [ 연습 문제 ]
# cancer data를 로지스틱 회귀로 분류하는 예측 모델링을 수행한 뒤 예측 정확도 확인!

# 1. data loading
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
cancer_x = cancer['data']
cancer_y = cancer['target']

# 2. data split
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(cancer_x, cancer_y)

# 3. modeling
from sklearn.linear_model import LogisticRegression
m_lr = LogisticRegression()
m_lr.fit(train_x, train_y)

# 평가
m_lr.score(test_x, test_y)    # 93.006

# 추정된 회귀 계수 확인
m_lr.coef_
m_lr.intercept_

# 예측값 확인
m_lr.predict_proba(test_x)    # 각 클래스별 예측 확률
m_lr.predict(test_x)          # 각 클래스별 예측 값(P(Y=0) > P(Y=1) => 0 결론!)


# [ 연관 분석 ]
# 비지도 학습
# 장바구니 분석 : 장바구니를 분석해서 장바구니가 가지고 있는 연관 규칙을 발견
# '맥주를 구매하는 고객은 주로 오징어를 구매하더라'라는 두 상품의 연관 규칙을 발견
# => 상품 추천, 번들 판매, 교차 판매, 판매 진열 변경, ...

# [ 장바구니 목록 ]
# 고객1 : [맥주, 오징어, 치즈]
# 고객2 : [소주, 맥주, 라면]
# 고객3 : [맥주, 오징어]
# 고객4 : [라면, 김치, 계란]
# 고객5 : [맥주, 소세지]

# 맥주를 구매하는 고객이 주로 구매하는 상품이 무엇인가?
# 2회 구매 : 오징어
# 1회 구매 : 치즈, 라면, 소주, 소세지 ...

# 평가 지표
# 1. 지지도 : 추천 데이터를 얼마나 지지할수 있느냐에 대한 정도(함께 구매한 비율이 높을수록 지지할만하다)
# P(A n B) = P(맥주 n 오징어) = 맥주와 오징어를 동시에 구매한 횟수 / 전체 장바구니 수 = 2/5

# 2. 신뢰도 : 추천 상품에 대한 신뢰도(추천에 대한 강도*)
# P(A|B) = P(오징어|맥주) = P(오징어 n 맥주) / P(맥주)
# case1 : 원래는 맥주를 산 횟수가 10회, 맥주랑 오징어를 동시에 구매한 횟수 2회  => 신뢰도 : (2/20)/(10/20) = 2/10
# case2 : 원래는 맥주를 산 횟수가 5회,  맥주랑 오징어를 동시에 구매한 횟수 2회  => 신뢰도 : 2/5
# 총 장바구니 수 : 20

# 3. 향상도 : 우연히 상품이 구매가 많아서 동시 구매가 많아질 확률을 재조정한 값
#             우연히 그 날 오징어 구매가 많아서 맥주와 오징어 동시 구매도 많아진 것일수 있음     
# P(A|B) / P(A) = P(오징어|맥주) / P(오징어)

# case1 : 원래는 맥주를 산 횟수가 10회, 맥주랑 오징어를 동시에 구매한 횟수 2회(오징어 5회)  => 2/10 / (5/20)
# case2 : 원래는 맥주를 산 횟수가 5회,  맥주랑 오징어를 동시에 구매한 횟수 2회(오징어 10회)  => 2/5 / (10/20)

# [ 참고 : 조건부 확률 ]
#       대출O 대출X 
# 스팸   48    2     50
# 햄     5     45    50
#        53    47
       
# 대출 => 스팸일 확률?
# P(스팸|대출) = P(스팸 n 대출) / P(대출) = (48/100)/(53/100) = 48/53
# P(A|B) = P(A n B) /  P(B)


# [ 예제 ]
# A편의점의 최근 1년 장바구니는 총 500개로 확인되었다.
# 그 중 각각의 구매 횟수가 다음과 같다고 가정하자.

# 군고구마 총 구매 횟수 : 50
# 군고구마 우유 구매 횟수 : 25
# 우유 총 구매 횟수 : 50

# 삼각깁밥 총 구매 횟수 : 100
# 삼각김밥, 컵라면 구매 횟수 : 80
# 컵라면 총 구매 횟수 : 90

# 불닭볶음면 총 구매 횟수 : 5
# 불닭볶음면, 스트링 치즈 구매 횟수 : 4
# 스트링 치즈 총 구매 횟수 : 10
#                                  지지도,  신뢰도,        향상도
# 군고구마 => 우유의 추천            25/500   25/50=0.5    0.5/(50/500)=5
# 삼각김밥 => 컵라면 추천            80/500   80/100=0.8   0.8/(90/500)=4.44
# 불닭볶음면 => 스트링 치즈 추천      4/500     4/5=0.8     0.8/(10/500)=40 


# [ 연관 분석 실습 ]
# 1. data loading
vlist = [['맥주', '오징어', '치즈'],  
         ['소주', '맥주', '라면'], 
         ['맥주', '오징어'], 
         ['라면', '김치', '계란'], 
         ['맥주', '소세지']]

# 참고 - 실 데이터 형식
# 구매아이디   날짜                품목
# 1          2001/01/01 14:00     계란    
# 1          2001/01/01 14:00     라면
# 2          2001/01/01 14:05     맥주
# 2          2001/01/01 14:05     소주
# 2          2001/01/01 14:05     김치

# 자 그럼 위 데이터에서는 맥주 -> 오징어에 대한 지지도, 신뢰도와 향상도는 몇 일까요?
# - 지지도 : P(맥주 n 오징어) = 2/5 = 0.4
# - 신뢰도 : P(오징어|맥주) = 2/4 = 0.5
#            P(오징어|맥주) = P(맥주 n 오징어) / P(맥주) 
#                          = (2/5) / (4/5) 
# - 향상도 : P(오징어|맥주) / P(오징어) = 0.5 / (2/5) = 1.25
                                             
# 2. modeling
# pip install mlxtend
from mlxtend.preprocessing import TransactionEncoder              # 트랜잭션데이터 변환용
from mlxtend.frequent_patterns import apriori, association_rules  # 모델링

# 데이터 변환
m_te = TransactionEncoder()
alist = m_te.fit(vlist).transform(vlist)

df_list = DataFrame(alist, columns = m_te.columns_ )

# 모델링
m_ap = apriori(df_list, min_support=0.1, use_colnames=True)
m_ap

df_result = association_rules(m_ap, metric='lift', min_threshold=1)
df_result.columns
df_result[['antecedents', 'consequents', 'support', 'confidence', 'lift']]


# [ 연습 문제 ]
# 식당 메뉴에 대한 주문 현황을 분석하여 추가 메뉴 추천(chipotle.tsv)
df1 = pd.read_csv('chipotle.tsv', sep='\t')
      

# =============================================================================
# ######### 파이썬분석_1112 필기
# =============================================================================
