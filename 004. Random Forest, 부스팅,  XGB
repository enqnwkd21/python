# Random Forest
# 의사결정나무가 여러개인 분류 모델
# 앙상블 알고리즘(여러 모델이 혼합되어 하나의 예측 결과를 가져다 주는 방식, 예.knn + pca)
# 복원 추출을 허용하여 원본 데이터셋에서 랜덤하게 선택된 데이터를 각 트리에 학습 = (부트스트랩)
# 각 트리의 결과를 결합하여(다수결 혹은 평균) 최종 결론을 내는 방식 = (배깅)
# 각 노드를 결정하는 설명 변수의 후보의 개수 - 임의노드최적화(in R : mtry, in python : max_features)
# 설명변수의 후보의 개수가 클수록 서로 비슷한 트리가 구성될 확률이 높다
# 설명변수의 후보의 개수가 클수록 트리 상관성이 높아진다(임의성 정도가 클수록 트리상관성이 높아짐)

# 0. module loading
from sklearn.ensemble import RandomForestClassifier as rf_c

import sklearn.ensemble
dir(sklearn.ensemble)

# 1. data loading, split
cancer = pd.read_csv('cancer.csv')
cancer_x = cancer.iloc[:,2:]
cancer_y = cancer.iloc[:,1]
train_x, test_x, train_y, test_y = train_test_split(cancer_x, cancer_y, random_state=0)

# 2. modeling
m_rf = rf_c()
m_rf.fit(train_x, train_y)
m_rf.score(train_x, train_y) # 100
m_rf.score(test_x, test_y)   # 97.90

# with CV
cv_score = cv(m_rf, cancer_x, cancer_y)
cv_score.mean()              # 96.31

# 3. tunning
# 1) max_depth : 트리의 크기, 클수록 복잡한 모델 생성(예측력 강화, but 과대적합 문제 발생 가능성 높아짐)
# 2) min_samples_split : 최소 가지치기 기준(작을수록 더 복잡한 모델 생성)
# 3) max_features : 임의성 정도(각 노드 결정시 설명변수의 후보 수, 클수록 서로 비슷한 트리 구성)
    
rf_c(n_estimators,      # 트리수(default : 100)
     criterion,         # 불순도(default : gini)
     max_depth,
     min_samples_split, # 가지치기를 할 최소 기준의 수
     max_features =,    # np.sqrt(n_features) 자동으로 설명 변수의 개수 기반으로 고정(1~설명변수개수)
     n_jobs = )         # 병렬처리를 위한 옵션  

vscore_tr =[];vscore_te =[]
for i in range(1,31) :
    m_rf = rf_c(max_features=i)
    m_rf.fit(train_x, train_y)
    vscore_tr.append(m_rf.score(train_x, train_y))
    vscore_te.append(m_rf.score(test_x, test_y))

import matplotlib.pyplot as plt
plt.plot(range(1,31), vscore_tr, label = 'train_score', c = 'r')
plt.plot(range(1,31), vscore_te, label = 'test_score', c = 'b')
plt.legend


# 4. 변수 중요도
cancer_x.columns
s1 = Series(m_rf.feature_importances_, index = cancer_x.columns)
s1.sort_values(ascending = False)

s1.plot(kind = 'barh')  # 'barh' 수평 bar plot

# =============================================================================
# 부스팅
# 이전 모델의 오차를 보완하는 학습 형태
# 약한 모델로 부터 강한 학습기를 만드는 과정
# 에이다부스트(AdaBoost), 그래디언트부스트(GradientBoosting)
# 에이다부스트 : 오분류 데이터에 가중치를 수정하여 이전 트리의 오차를 보완하는 헝태
# 그래디언트부스트 : 사전 가지치기를 통해 이전 트리의 오차를 보완하는 형태 (오차를 최소화하는 가지치기를 반복)

from sklearn.ensemble import GradientBoostingClassifier as gbm

# 1. data loading, split
cancer = pd.read_csv('cancer.csv')
cancer_x = cancer.iloc[:,2:]
cancer_y = cancer.iloc[:,1]
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(cancer_x, cancer_y, random_state=0)

# 2. modeling
m_gbm = gbm()
m_gbm.fit(train_x, train_y)
m_gbm.score(test_x, test_y) # 97.20

# 3. tunning
# 1) n_estimators : 트리 수
vscore_tr =[];vscore_te =[]
for i in range(1,101) :
    m_gbm = gbm(n_estimators=i)
    m_gbm.fit(train_x, train_y)
    vscore_tr.append(m_gbm.score(train_x, train_y))
    vscore_te.append(m_gbm.score(test_x, test_y))
    
import matplotlib.pyplot as plt
plt.plot(range(1,101), vscore_tr, label = 'train_score', c = 'r')
plt.plot(range(1,101), vscore_te, label = 'test_score', c = 'b')
plt.legend()


from sklearn.model_selection import cross_val_score as cv
vscore = []
for i in range(1,101) :
    m_gbm = gbm(n_estimators=i)
    m_gbm.fit(train_x, train_y)
    cv_result = cv(m_gbm, cancer_x, cancer_y, cv = 5)
    vscore.append(cv_result.mean())
    
plt.plot(range(1,101), vscore, c = 'b')
plt.legend()   


m_gbm = gbm(n_estimators=20)
m_gbm.fit(train_x, train_y)
m_gbm.score(test_x, test_y)  # 96.50


# 2) learning_rate : 학습률 (0 ~ 1 사이값)
# 오차 보완 정도
# 클수록 오차 보완 정도가 강화

vscore_tr =[];vscore_te =[]
for i in [0.001, 0.01, 0.1, 1] :
    m_gbm = gbm(learning_rate=i)
    m_gbm.fit(train_x, train_y)
    vscore_tr.append(m_gbm.score(train_x, train_y))
    vscore_te.append(m_gbm.score(test_x, test_y))
    
import matplotlib.pyplot as plt
plt.plot([0.001, 0.01, 0.1, 1], vscore_tr, label = 'train_score', c = 'r')
plt.plot([0.001, 0.01, 0.1, 1], vscore_te, label = 'test_score', c = 'b')
plt.legend()

# -- best : 0.1

# =============================================================================
# XGB
# pip install xgboost
from xgboost.sklearn import XGBClassifier as xgb


# 변수 스케일링
# 설명변수(x만 스케일링)의 서로 다른 범위(분산)을 동일한 범주내에서 비교하기 위한 목적
# 거리기반 모델, 계수 크기 비교, 신경망, PCA, SVM 등의 모델 필요
# standard scaling : (x - x.mean()) / x.std()
# minmax scaling : (x - x.min()) / (x.max() - x.min())

from sklearn.preprocessing import StandardScaler as standard
from sklearn.preprocessing import MinMaxScaler as minmax

# cancer data set scaling
m_sc = standard()
m_sc.fit(cancer_x)        # 스케일링에 필요한 정보 연산(평균, 표준편차), 실제 x 데이터는 아직 변환 안됨
m_sc.transform(cancer_x)  # 위 데이터를 사용하여 실제 변환 수행
cancer_x_sc = m_sc.transform(cancer_x)

# scaling시 주의 사항
# 이미 train / test data set이 분리된 상황

# case1) 서로 같은 기준으로 train / test 적용 or 아예 애초에 원본을 먼저 스케일링을 하고 나누기
m_sc = minmax()
m_sc.fit(train_x)
train_x_sc = m_sc.transform(train_x)
test_x_sc = m_sc.transform(test_x)

train_x_sc.min()  # 0
train_x_sc.max()  # 1

test_x_sc.min()   # -0.00079 : 0으로 규격화 X
test_x_sc.max()   # 1.36 : 1으로 규격화 X

# case2) 서로 같은 기준으로 train / test 각각 적용 _ 잘못된 방법
m_sc1 = minmax()
m_sc2 = minmax()

m_sc1.fit(train_x)
m_sc2.fit(test_x)

train_x_sc2 = m_sc1.transform(train_x)
test_x_sc2 = m_sc2.transform(test_x)

train_x_sc2.min()  # 0으로 규격화
test_x_sc2.max()   # 1으로 규격화

test_x_sc2.min()   # 0으로 규격화 
train_x_sc2.max()  # 1으로 규격화

# 올바른 스케일링 기법 비교를 위한 시각화
# 1) fig, subplot 생성
fig, ax = plt.subplots(1,3)

# 2) 원본 데이터 산점도
# 스케일링 작업은 원본 데이터가 갖는 본질적 특성(데이터간의 거리)은 유치한 채
# 데이터의 범위만 수정하는 작업이므로 
# 올바른 스케일링 후 데이터는 원본의 분포(산점도)와 동일해야 함  

import mglearn     # pip install mglearn으로 설치 필요
plt.rc('font', family = 'Malgun Gothic')

fig, ax = plt.subplots(1,3)
ax[0].scatter(train_x.iloc[:,0], train_x.iloc[:,1], c=mglearn.cm2(0), label = 'train')
ax[0].scatter(test_x.iloc[:,0], test_x.iloc[:,1], c=mglearn.cm2(1), label = 'test')
ax[0].legend()
ax[0].set_title('원본 산점도')
ax[0].set_xlabel(train_x.columns[0])
ax[0].set_ylabel(train_x.columns[1])

# 3) 올바른 스케일링 데이터 산점도
ax[1].scatter(train_x_sc[:,0], train_x_sc[:,1], c=mglearn.cm2(0), label = 'train')
ax[1].scatter(test_x_sc[:,0], test_x_sc[:,1], c=mglearn.cm2(1), label = 'test')
ax[1].legend()
ax[1].set_title('올바른 스케일링 산점도', fontsize = 10)
ax[1].set_xlabel(train_x.columns[0])
ax[1].set_ylabel(train_x.columns[1])

# 4) 잘못된 스케일링 데이터 산점도
# 원본의 데이터가 가지고 있는 데이터간의 특징(속성, 거리)을 그대로 유지하지 못한 체 스케일링이 진행 됨
ax[2].scatter(train_x_sc2[:,0], train_x_sc2[:,1], c=mglearn.cm2(0), label = 'train')
ax[2].scatter(test_x_sc2[:,0], test_x_sc2[:,1], c=mglearn.cm2(1), label = 'test')
ax[2].legend()
ax[2].set_title('잘못된 스케일링 산점도', fontsize = 10)
ax[2].set_xlabel(train_x.columns[0])
ax[2].set_ylabel(train_x.columns[1])
